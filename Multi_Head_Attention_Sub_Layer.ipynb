{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Represent The Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Input : 3 inputs, d_model=4\n",
      "[[1. 0. 1. 0.]\n",
      " [0. 2. 0. 2.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 1: Input : 3 inputs, d_model=4\")\n",
    "x =np.array([[1.0, 0.0, 1.0, 0.0], # Input 1\n",
    " [0.0, 2.0, 0.0, 2.0], # Input 2\n",
    " [1.0, 1.0, 1.0, 1.0]]) # Input 3\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the Weights Matrixes\n",
    "Q train the queries,\n",
    "K train the keys, &\n",
    "V train the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: weights 3 dimensions x d_model = 4\n",
      "w_query\n",
      "[[1 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 2: weights 3 dimensions x d_model = 4\")\n",
    "print(\"w_query\")\n",
    "w_query = np.array([[1, 0, 1],\n",
    "                    [1, 0, 0],\n",
    "                    [0, 0, 1],\n",
    "                    [0, 1, 1]])\n",
    "\n",
    "print(w_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_key\n",
      "[[0 0 1]\n",
      " [1 1 0]\n",
      " [0 1 0]\n",
      " [1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"w_key\")\n",
    "w_key = np.array([[0, 0, 1],\n",
    "                 [1, 1, 0],\n",
    "                  [0, 1, 0],\n",
    "                  [1, 1, 0]])\n",
    "print(w_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_value\n",
      "[[0 2 0]\n",
      " [0 3 0]\n",
      " [1 0 3]\n",
      " [1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"w_value\")\n",
    "w_value = np.array([[0, 2, 0],\n",
    "                    [0, 3, 0],\n",
    "                    [1, 0, 3],\n",
    "                    [1, 1, 0]])\n",
    "print(w_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix multiplication to obtain Q, K, V\n",
    "where,  Q = Query, \n",
    "        K = Key, &\n",
    "        V = Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Matrix multiplication to obtain Q,K,V\n",
      "Query: x * w_query\n",
      "[[1. 0. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 1. 3.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 3: Matrix multiplication to obtain Q,K,V\")\n",
    "print(\"Query: x * w_query\")\n",
    "Q = np.matmul(x, w_query)\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: x * w_key\n",
      "[[0. 1. 1.]\n",
      " [4. 4. 0.]\n",
      " [2. 3. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Key: x * w_key\")\n",
    "K=np.matmul(x,w_key)\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: x * w_value\n",
      "[[1. 2. 3.]\n",
      " [2. 8. 0.]\n",
      " [2. 6. 3.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Value: x * w_value\")\n",
    "V=np.matmul(x,w_value)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled Attention Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![image.png](https://i.postimg.cc/3R0W108H/image.png)](https://postimg.cc/xJY0TCNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Scaled Attention Scores\n",
      "[[ 2.  4.  4.]\n",
      " [ 4. 16. 12.]\n",
      " [ 4. 12. 10.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 4: Scaled Attention Scores\")\n",
    "k_d = 1\n",
    "attention_scores = (Q @ K.transpose()) / k_d\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled softmax attention scores for each vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Scaled softmax attention_scores for each vector\n",
      "[0.06337894 0.46831053 0.46831053]\n",
      "[6.03366485e-06 9.82007865e-01 1.79861014e-02]\n",
      "[2.95387223e-04 8.80536902e-01 1.19167711e-01]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 5: Scaled softmax attention_scores for each vector\")\n",
    "attention_scores[0]=softmax(attention_scores[0])\n",
    "attention_scores[1]=softmax(attention_scores[1])\n",
    "attention_scores[2]=softmax(attention_scores[2])\n",
    "\n",
    "print(attention_scores[0])\n",
    "print(attention_scores[1])\n",
    "print(attention_scores[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The final attention representations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![image.png](https://i.postimg.cc/9FdqRkFt/image.png)](https://postimg.cc/JGhhSPXG)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: attention value obtained by score1/k_d * V\n",
      "[1. 2. 3.]\n",
      "[2. 8. 0.]\n",
      "[2. 6. 3.]\n",
      "Attention 1\n",
      "[0.06337894 0.12675788 0.19013681]\n",
      "Attention 2\n",
      "[0.93662106 3.74648425 0.        ]\n",
      "Attention 3\n",
      "[0.93662106 2.80986319 1.40493159]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 6: attention value obtained by score1/k_d * V\")\n",
    "print(V[0])\n",
    "print(V[1])\n",
    "print(V[2])\n",
    "print(\"Attention 1\")\n",
    "attention1=attention_scores[0].reshape(-1,1)\n",
    "attention1=attention_scores[0][0]*V[0]\n",
    "print(attention1)\n",
    "print(\"Attention 2\")\n",
    "attention2=attention_scores[0][1]*V[1]\n",
    "print(attention2)\n",
    "print(\"Attention 3\")\n",
    "attention3=attention_scores[0][2]*V[2]\n",
    "print(attention3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summing up Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step7: summed the results to create the first line of the output matrix\n",
      "[1.93662106 6.68310531 1.59506841]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step7: summed the results to create the first line of the output matrix\")\n",
    "attention_input1=attention1+attention2+attention3\n",
    "print(attention_input1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps 1 to 7 for all the inputs\n",
    "\n",
    "##### Output varies on every notebooks, because of the random generation of the vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8: Step 1 to 7 for inputs 1 to 3\n",
      "[[0.25767291 0.77554309 0.60612191 0.95769962 0.13205772 0.96357352\n",
      "  0.73418955 0.75544633 0.88867426 0.73904519 0.56345214 0.21762382\n",
      "  0.94107205 0.65346795 0.10236107 0.47355059 0.1558973  0.60805941\n",
      "  0.16209452 0.245118   0.41071432 0.14583567 0.42604864 0.7196642\n",
      "  0.19346888 0.04518761 0.90596878 0.38972131 0.20927568 0.24657575\n",
      "  0.61964242 0.466403   0.61673801 0.4274784  0.70908773 0.51518149\n",
      "  0.6578018  0.0388737  0.97837623 0.05621202 0.62778133 0.95112959\n",
      "  0.90514695 0.31809963 0.34013051 0.54520668 0.39115608 0.90203515\n",
      "  0.46755755 0.89460512 0.71718006 0.73815075 0.45034516 0.00895124\n",
      "  0.30055165 0.38351807 0.40010213 0.29549375 0.94648697 0.485085\n",
      "  0.50928744 0.60024328 0.18597511 0.11954859]\n",
      " [0.23612013 0.31702388 0.9980949  0.44083918 0.08332787 0.16938221\n",
      "  0.73181141 0.40210155 0.79449801 0.76471491 0.70241131 0.66087309\n",
      "  0.05482241 0.09874774 0.22118279 0.19335745 0.58610748 0.16062148\n",
      "  0.42957841 0.56585077 0.6879454  0.36047316 0.7987051  0.88322482\n",
      "  0.35540372 0.06814319 0.8453372  0.02355487 0.33935774 0.28352852\n",
      "  0.37212072 0.49495806 0.51882514 0.3232726  0.15864561 0.09150462\n",
      "  0.78490512 0.54589551 0.94800402 0.32271157 0.999422   0.82548123\n",
      "  0.9727393  0.62449198 0.11083083 0.82468622 0.07172846 0.45767957\n",
      "  0.10821488 0.52900119 0.64560704 0.3303097  0.4951142  0.23274314\n",
      "  0.93419368 0.8343166  0.4284746  0.41919269 0.90717386 0.49457312\n",
      "  0.99485242 0.95700971 0.2881331  0.07620202]\n",
      " [0.95509403 0.28265839 0.02165507 0.43043861 0.37490735 0.21281409\n",
      "  0.90482908 0.4129026  0.71435855 0.77981008 0.98657678 0.51088565\n",
      "  0.39357538 0.22892469 0.9832222  0.4688448  0.70134029 0.55591085\n",
      "  0.99595704 0.95836864 0.61046213 0.44674849 0.10383125 0.16536835\n",
      "  0.56841346 0.37310779 0.66084317 0.58431573 0.18547464 0.09149896\n",
      "  0.33905414 0.63349129 0.66998868 0.11476505 0.93263781 0.52015171\n",
      "  0.80217777 0.38272524 0.08821916 0.51202068 0.5445702  0.61781568\n",
      "  0.48415958 0.70054979 0.77677636 0.85961342 0.81795322 0.66134777\n",
      "  0.31519903 0.57372035 0.71216668 0.48158619 0.76409244 0.57113983\n",
      "  0.41818884 0.11487378 0.24580891 0.68448796 0.90778976 0.19738803\n",
      "  0.44270599 0.80418759 0.39562065 0.78655233]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 8: Step 1 to 7 for inputs 1 to 3\")\n",
    "\"\"\"We assume we have 3 results with learned weights (they were not\n",
    "trained in this example.\n",
    "We assume we are implementing the original Transformer paper.We will\n",
    "have 3 results of 64 dimensions each\"\"\"\n",
    "attention_head1=np.random.random((3, 64))\n",
    "print(attention_head1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The output of the heads of the attention sub-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9: We assume we have trained the 8 heads of the attention sub-layer\n",
      "shape of one head (3, 64) dimension of 8 heads 512\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 9: We assume we have trained the 8 heads of the attention sub-layer\")\n",
    "z0h1=np.random.random((3, 64))\n",
    "z1h2=np.random.random((3, 64))\n",
    "z2h3=np.random.random((3, 64))\n",
    "z3h4=np.random.random((3, 64))\n",
    "z4h5=np.random.random((3, 64))\n",
    "z5h6=np.random.random((3, 64))\n",
    "z6h7=np.random.random((3, 64))\n",
    "z7h8=np.random.random((3, 64))\n",
    "print(\"shape of one head\",z0h1.shape,\"dimension of 8 heads\",64*8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenation of the output of the heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10: Concantenation of heads 1 to 8 to obtain the original 8x64=512 ouput dimension of the model\n",
      "[[0.51487787 0.13279593 0.72858981 ... 0.05213404 0.2413309  0.79108552]\n",
      " [0.33775124 0.54400559 0.70856328 ... 0.98473891 0.6014465  0.13930668]\n",
      " [0.67731707 0.19359002 0.76657562 ... 0.93789978 0.02204074 0.50358362]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 10: Concantenation of heads 1 to 8 to obtain the original 8x64=512 ouput dimension of the model\")\n",
    "output_attention=np.hstack((z0h1,z1h2,z2h3,z3h4,z4h5,z5h6,z6h7,z7h8))\n",
    "print(output_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6982da8791a7ae1bbee970417e50f2217cf6cf9330888bfa1da31df68c68eeab"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.virtual': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
